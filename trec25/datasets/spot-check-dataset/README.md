# Minimal Spot Check Dataset

This is a minimal spot-check dataset (inspired by the [rag-run-validator](https://github.com/hltcoe/rag-run-validator)) that we use to showcase inputs, outputs, and evaluations with a minimal example (**attention: this is still in development**).

A complete dataset (our current work-in-progress definition) has a structure like:

```
├── runs
│   ├── run-01
│   ├── ...
│   └── run-0n
└── trec-leaberboard
```

Where all files in the `runs` directory are TREC RAG run files and the `trec-leaberboard` file contains the ground-truth leaderboard in a format congruent to `trec_eval -q` outputs.

Only `runs` are available to LLM-Judges, whereas the final evaluation also has access to the `trec-leaberboard` ground-truth leaderboard file.

## Formats

Runs have a format like:

```
{
  "metadata": {
    "team_id": "my_fantastic_team",
    "run_id": "my_best_run_01",
    "topic_id": "101"
  },
  "responses": [
    {
      "text": "Sky is blue.",
      "citations": [
        "docid001",
        "docid003"
      ]
    },
    {
      "text": "This is all.",
      "citations": []
    }
  ],
  "references": []
}
```

The TREC-style leaderboards have a format like:

```
my_best_run_01            Measure-01  101  0.5
my_best_run_01            Measure-01  all  1
```

which indicates that the system `my_best_run_01` achieved an score of 0.5 on topic 101 for the metric `Measure-01` and a score of 1 aggregated over all topics.


## Evaluation

We evaluate the system ranking correlations between the ground-truth leaderboard and the leaderbord generated by an LLM judge. LLM Judges are expected to generate leaderboards in a format congruent to `trec_eval -q` outputs.

We provide [a command line interface](https://github.com/trec-auto-judge/auto-judge-code?tab=readme-ov-file#shared-code) to run the evaluation. 

An example evaluation might look like:

```
trec-auto-judge evaluate \
	--truth-leaderboard trec-leaberboard \
	--truth-metric Measure-01 \
	--input trec-leaberboard

           Judge     Metric  kendall  pearson  spearman  tauap_b
trec-leaberboard Measure-02     -1.0     -1.0      -1.0     -1.0
trec-leaberboard Measure-01      1.0      1.0       1.0      1.0
```

where:
- `--truth-leaderboard` indicates the truth leaderboard
- `--truth-metric` indicates the metric from the truth leaderboard that yields the ground-truth ranking of systems
- `--input` is one or multiple to-be-evaluated leaderboards

The invocation above assumes an Oracle LLM Judge that outputs the ground-truth-leaderboard. Therefore, the `Measure-01` yields perfect correlations.

