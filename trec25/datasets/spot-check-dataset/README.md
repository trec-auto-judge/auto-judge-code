---
configs:
- config_name: inputs
  data_files:
  - split: train
    path: ["runs/*.jsonl", "queries.jsonl", "corpus.jsonl.gz"]
- config_name: truths
  data_files:
  - split: train
    path: ["trec-leaderboard.txt"]

tira_configs:
  resolve_inputs_to: "."
  resolve_truths_to: "."
  baseline:
    link: https://github.com/trec-auto-judge/auto-judge-code/tree/main/trec25/judges/naive
    command: /naive-baseline.py --rag-responses $inputDataset/runs --output $outputDir/trec-leaderboard.txt
    format:
      name: ["trec-eval-leaderboard"]
  input_format:
    name: "trec-rag-runs"
  truth_format:
    name: "trec-eval-leaderboard"
  evaluator:
    image: ghcr.io/trec-auto-judge/auto-judge-code/cli:0.0.1
    command: trec-auto-judge evaluate --input ${inputRun}/trec-leaderboard.txt --aggregate --output ${outputDir}/evaluation.prototext
ir_dataset:
#  ir_datasets_id: "cranfield"
  directory: "."
---

# Minimal Spot Check Dataset

This is a minimal spot-check dataset (inspired by the [rag-run-validator](https://github.com/hltcoe/rag-run-validator)) that we use to showcase inputs, outputs, and evaluations with a minimal example (**attention: this is still in development**). This example dataset contains responses to the Cranfield topic 28 on "what application has the linear theory of curved wings" that reference two documents from the Cranfield corpus.

A complete dataset (our current work-in-progress definition) has a structure like:

```
├── runs
│   ├── run-01
│   ├── ...
│   └── run-0n
└── trec-leaderboard
```

Where all files in the `runs` directory are TREC RAG run files and the `trec-leaberboard` file contains the ground-truth leaderboard in a format congruent to `trec_eval -q` outputs.

Only `runs` are available to LLM-Judges, whereas the final evaluation also has access to the `trec-leaberboard` ground-truth leaderboard file.

## Formats

Runs have a format like:

```
{
  "metadata": {
    "team_id": "my_fantastic_team",
    "run_id": "my_best_run_01",
    "topic_id": "28"
  },
  "responses": [
    {
      "text": "Sky is blue.",
      "citations": [
        "docid001",
        "docid003"
      ]
    },
    {
      "text": "This is all.",
      "citations": []
    }
  ],
  "references": []
}
```

The TREC-style leaderboards have a format like:

```
my_best_run_01            Measure-01  28  0.5
my_best_run_01            Measure-01  all  1
```

which indicates that the system `my_best_run_01` achieved an score of 0.5 on topic 28 for the metric `Measure-01` and a score of 1 aggregated over all topics.


## Evaluation

We evaluate the system ranking correlations between the ground-truth leaderboard and the leaderbord generated by an LLM judge. LLM Judges are expected to generate leaderboards in a format congruent to `trec_eval -q` outputs.

We provide [a command line interface](https://github.com/trec-auto-judge/auto-judge-code?tab=readme-ov-file#shared-code) to run the evaluation. 

An example evaluation might look like:

```
trec-auto-judge evaluate \
	--truth-leaderboard trec-leaderboard.txt \
	--truth-metric Measure-01 \
	--input trec-leaderboard.txt

           Judge     Metric  kendall  pearson  spearman  tauap_b
trec-leaderboard Measure-02     -1.0     -1.0      -1.0     -1.0
trec-leaderboard Measure-01      1.0      1.0       1.0      1.0
```

where:
- `--truth-leaderboard` indicates the truth leaderboard
- `--truth-metric` indicates the metric from the truth leaderboard that yields the ground-truth ranking of systems
- `--input` is one or multiple to-be-evaluated leaderboards

The invocation above assumes an Oracle LLM Judge that outputs the ground-truth-leaderboard. Therefore, the `Measure-01` yields perfect correlations.

# Admin Section

The following contains the hugging-face format definition of this dataset that you can use to declaratively upload the dataset with evaluation configuration to tira. Step by step guides on how to upload are in [../README.md](../README.md):

